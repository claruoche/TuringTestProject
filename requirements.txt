We trained on learning_rate=2e-5, num_train_epochs=2, weight_decay=0.01. We used a Google colab environment with CUDA enabled PyTorch, 
huggingface transformers and data, in a python 3.1 environment. Our model architecture was a base transformer encoder, mean pooling/cls token,
and then a linear classifier layer for binary outputs.
